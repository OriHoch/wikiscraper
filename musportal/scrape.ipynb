{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{'python3 -m pip install -U dataflows'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time, json\n",
    "from dataflows import Flow, checkpoint, printer\n",
    "\n",
    "PAGE_URL_TEMPLATE = 'http://www.museumsinisrael.gov.il/_Layouts/15/Tmit.SP2013.Mozionim.UI/GetSearch.ashx?type=items&Culture=he-IL&refiners=HebArtDomainName%3A%22%D7%90%D7%9E%D7%A0%D7%95%D7%AA%22&page={page}'\n",
    "ITEM_URL_TEMPLATE = 'http://www.museumsinisrael.gov.il/he/items/Pages/ItemCard.aspx?IdItem={link}'\n",
    "\n",
    "session = requests.Session()\n",
    "\n",
    "def get_page_items(page, retry_num=0):\n",
    "    if retry_num > 10:\n",
    "        raise Exception('too many retries for page {}'.format(page))\n",
    "    url = PAGE_URL_TEMPLATE.format(page=page)\n",
    "    try:\n",
    "        res = session.get(url).json()\n",
    "    except Exception:\n",
    "        time.sleep(15)\n",
    "        print('page {} retry_num {}'.format(page, retry_num+1))\n",
    "        return get_page_items(page, retry_num+1)\n",
    "    return res\n",
    "\n",
    "def get_all_page_items():\n",
    "    total_found = 0\n",
    "    num_yielded = 0\n",
    "    for page in range(0,800):\n",
    "        print(page)\n",
    "        res = get_page_items(page)\n",
    "        assert len(res) == 1\n",
    "        assert res[0]['Type'] == 'פריטים'\n",
    "        total_found = res[0]['TotalFound']\n",
    "        cur_page_num_yielded = 0\n",
    "        for item in res[0]['items']:\n",
    "            row = {str(k): str(v) for k, v in item.items()}\n",
    "            row['item_url'] = ITEM_URL_TEMPLATE.format(link=row.pop('Link'))\n",
    "            yield row\n",
    "            num_yielded += 1\n",
    "            cur_page_num_yielded += 1\n",
    "        print(num_yielded)\n",
    "        if cur_page_num_yielded < 1:\n",
    "            break\n",
    "        time.sleep(2)\n",
    "\n",
    "Flow(\n",
    "    get_all_page_items(),\n",
    "    checkpoint('all_page_items'),\n",
    "    printer(tablefmt='html', num_rows=1)\n",
    ").process()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download item pages\n",
    "\n",
    "The following takes ~4-5 days\n",
    "\n",
    "```\n",
    "python3 musportal/download_item_pages.py\n",
    "```\n",
    "\n",
    "Inspect the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflows import Flow, load, printer, filter_rows\n",
    "\n",
    "Flow(\n",
    "    load('../data/musportal-item-pages/datapackage.json'),  printer(tablefmt='html', num_rows=1),\n",
    "    filter_rows(not_equals=[{\"downloaded_status_code\": 200}]), printer(tablefmt='html', num_rows=1),\n",
    "    filter_rows(not_equals=[{\"downloaded_status_code\": 502}]), printer(tablefmt='html', num_rows=1),\n",
    ").process()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse the item pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{'python3 -m pip install -U requests-html'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflows import Flow, load, printer, checkpoint, add_field\n",
    "from requests_html import HTMLSession\n",
    "from retrying import retry\n",
    "import datetime\n",
    "from datapackage import Package\n",
    "import os\n",
    "from requests_html import HTML\n",
    "from time import sleep\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def super_strip(string):\n",
    "    return string.strip().strip(':').strip()\n",
    "\n",
    "FIELDS = {'artist_names': 'string',\n",
    "          'main_image_url': 'string',\n",
    "          'אורך': 'string',\n",
    "          'רוחב': 'string',\n",
    "          'מוזאון': 'string',\n",
    "          'תחום': 'string',\n",
    "          'סיווג': 'string',\n",
    "          'טכניקה': 'string',\n",
    "          'קוד פריט': 'string',\n",
    "          'צלמים': 'string',\n",
    "          'מקום': 'string',\n",
    "          'תאריך': 'string',\n",
    "          'קרדיט': 'string',\n",
    "          'extra': 'array',\n",
    "          'תקופה': 'string',\n",
    "          'description': 'string',\n",
    "          'image_urls': 'array'}\n",
    "\n",
    "for extras in [[['תקופה'], ['תאריך סיום'], ['צבע'], ['גובה'], ['קוטר'], ['חומר'], ['אוצר/ת']],]:\n",
    "    for extra in extras:\n",
    "        if extra[0] not in FIELDS.keys():\n",
    "            FIELDS[extra[0]] = 'string'\n",
    "\n",
    "for extra in ['מלות מפתח', 'זכויות יוצרים', 'מספר רישום',\n",
    "              'תיאור פנים', 'עובי', 'רוחב 299', 'רוחב 204',\n",
    "              'אסכולה', 'עומק', 'קוטר מקסימלי', 'בעלות', 'רוחב 313', 'תיאור גב', 'מעצב', 'מידע נוסף',\n",
    "              'תולדות היוצר/אמן', 'מספר בנקודת האיסוף בוויסבאדן', 'סגנון', 'אורך 296', 'אורך 203',\n",
    "              'מזמין', 'רוחב מקסימלי', 'גובה 300', 'אורך 312', 'הערות', 'רוחב 297']:\n",
    "    if extra not in FIELDS.keys():\n",
    "        FIELDS[extra] = 'string'\n",
    "\n",
    "all_extra_keys = set()\n",
    "            \n",
    "def set_row_field(row, k, v):\n",
    "    if k in FIELDS:\n",
    "        row[k] = v\n",
    "    else:\n",
    "        row['extra'].append([k, v])\n",
    "        all_extra_keys.add(k)\n",
    "\n",
    "def scrape_item_pages():\n",
    "    mutportal_item_descriptions = []\n",
    "\n",
    "    def _scrape(rows):\n",
    "        yielded_rows = 0\n",
    "        for i, row in enumerate(rows):\n",
    "            row['extra'] = []\n",
    "            row['image_urls'] = []\n",
    "            item_url = row['item_url']\n",
    "            yielded_rows += 1\n",
    "            html = None\n",
    "            if row['downloaded_status_code'] == 200:\n",
    "                with open('../' + row['downloaded_file_name']) as f:\n",
    "                    html = HTML(html=f.read())\n",
    "            if not html or 'Website under construction' in html.html:\n",
    "                print(f'{item_url}: missing item')\n",
    "                for n, t in FIELDS.items():\n",
    "                    row[n] = '' if t == 'string' else None\n",
    "            else:\n",
    "                item_main_pics = html.find('.ItemMainPic')\n",
    "                assert len(item_main_pics) == 1\n",
    "                imgs = item_main_pics[0].find('img')\n",
    "                try:\n",
    "                    for img in imgs:\n",
    "                        row['image_urls'].append(imgs[0].attrs['src'])                    \n",
    "                except Exception as e:\n",
    "                    print(f'{item_url}: exception parsing main image urls: {e}')\n",
    "                    row['image_urls'] = []\n",
    "                articles = html.find('article.ItmeDetailsZone')\n",
    "                assert len(articles) == 1\n",
    "                article = articles[0]\n",
    "                description = \"\\n\".join((item_description_allinfo.text for item_description_allinfo \n",
    "                                         in article.find('.ItemDescripion .allinfo')))\n",
    "                mutportal_item_descriptions.append({'item_url': row['item_url'],\n",
    "                                                    'SecendRow': row['SecendRow'], \n",
    "                                                    'therdRow': row['therdRow'],\n",
    "                                                    'description': description})\n",
    "                detail_infos = article.find('div.detailInfo')\n",
    "                for detail_info in detail_infos:\n",
    "                    item_label_names = detail_info.find('.itemlablename')\n",
    "                    if len(item_label_names) == 0: continue\n",
    "                    assert len(item_label_names) == 1, detail_info.html\n",
    "                    item_label_name = super_strip(item_label_names[0].text)\n",
    "                    item_text_names = detail_info.find('.itemTextname')\n",
    "                    size_tables = detail_info.find('.sizeTable')\n",
    "                    ic_artist_list = detail_info.find('.ICArtiistList')\n",
    "                    all_infos = detail_info.find('.allinfo')\n",
    "                    if len(item_text_names) == 1:\n",
    "                        assert len(size_tables) == 0 and len(ic_artist_list) == 0 and len(all_infos) == 0, detail_info.html\n",
    "                        assert len(item_text_names) == 1, detail_info.html\n",
    "                        set_row_field(row, item_label_name, item_text_names[0].text.strip())\n",
    "                    elif len(size_tables) == 1:\n",
    "                        assert len(item_text_names) == 0 and len(ic_artist_list) == 0 and len(all_infos) == 0, detail_info.html\n",
    "                        trs = size_tables[0].find('tr')\n",
    "                        for tr in trs:\n",
    "                            meas_labels = tr.find('.MeasLabel')\n",
    "                            meas_values = tr.find('.MeasValue')\n",
    "                            assert len(meas_labels) == 1 and len(meas_values) == 1, detail_info.html\n",
    "                            meas_label = super_strip(meas_labels[0].text)\n",
    "                            set_row_field(row, meas_label, super_strip(meas_values[0].text))\n",
    "                    elif len(ic_artist_list) == 1:\n",
    "                        assert len(size_tables) == 0 and len(item_text_names) == 0 and len(all_infos) == 0, detail_info.html\n",
    "                        artist_names = []\n",
    "                        for ic_artist_name in ic_artist_list[0].find('.ICArtistName'):\n",
    "                            artist_names.append(super_strip(ic_artist_name.text))\n",
    "                        set_row_field(row, 'artist_names', ', '.join(artist_names))\n",
    "                    elif len(all_infos) == 1:\n",
    "                        set_row_field(row, item_label_name, all_infos[0].text.strip())\n",
    "                    else:\n",
    "                        raise Exception(detail_info.html)\n",
    "            yield row\n",
    "        \n",
    "    def _split_description(package):\n",
    "        descriptor = package.pkg.descriptor\n",
    "        assert len(descriptor['resources']) == 1\n",
    "        descriptor['resources'][0].update(**{'name': 'musportal_items', 'path': 'musportal_items.csv',\n",
    "                                             'schema': {'fields': (descriptor['resources'][0]['schema']['fields'] \n",
    "                                                                   + [{'name': n, 'type': t}\n",
    "                                                                      for n, t in FIELDS.items()])}})\n",
    "        descriptor['resources'].append({'name': 'musportal_item_descriptions', 'path': 'musportal_item_descriptions.csv',\n",
    "                                        'schema': {'fields': [{'name': 'item_url', 'type': 'string'},\n",
    "                                                              {'name': 'SecendRow', 'type': 'string'},\n",
    "                                                              {'name': 'therdRow', 'type': 'string'},\n",
    "                                                              {'name': 'description', 'type': 'string'}]}})\n",
    "        yield Package(descriptor)\n",
    "        for i, resource in enumerate(package):\n",
    "            assert i == 0\n",
    "            yield _scrape(resource)\n",
    "        yield (row for row in mutportal_item_descriptions)\n",
    "    \n",
    "    return _split_description\n",
    "\n",
    "!{'rm -rf .checkpoints/all_items'}\n",
    "\n",
    "Flow(\n",
    "    # checkpoint('all_page_items'),\n",
    "    load('../data/musportal-item-pages/datapackage.json'),\n",
    "    scrape_item_pages(),\n",
    "    checkpoint('all_items'),\n",
    "    printer(tablefmt='html', num_rows=1)\n",
    ").process()[1]\n",
    "\n",
    "print(f'extra keys = {all_extra_keys}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
