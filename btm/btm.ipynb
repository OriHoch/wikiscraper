{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{'python3 -m pip install -U dataflows'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflows import Flow, load, printer, checkpoint, add_field\n",
    "from requests_html import HTMLSession\n",
    "from retrying import retry\n",
    "import datetime\n",
    "from datapackage import Package\n",
    "import os\n",
    "from requests_html import HTML\n",
    "import urllib.parse\n",
    "import traceback\n",
    "from google.cloud import storage\n",
    "import json\n",
    "from time import sleep\n",
    "\n",
    "import sys\n",
    "print('redirecting stdout and stderr to console')\n",
    "sys.stdout = open('/dev/stdout', 'w')\n",
    "sys.stderr = open('/dev/stderr', 'w')\n",
    "\n",
    "MAX_OBJECTS = 0\n",
    "\n",
    "session = HTMLSession()\n",
    "bucket = storage.Client().get_bucket(\"wmil-1946\")\n",
    "    \n",
    "# wait 4s, 8s, 16s, 32s and continue with 32s up to 5m\n",
    "@retry(wait_exponential_multiplier=4000, wait_exponential_max=32000, stop_max_delay=30000)\n",
    "def retry_session_get(*args, **kwargs):\n",
    "    print('session_get: ' + str(args) + ' ' + str(kwargs))\n",
    "    try:\n",
    "        r = session.get(*args, **kwargs)\n",
    "        sleep(.2)\n",
    "        return r\n",
    "    except Exception as e:\n",
    "        print(f\"{e}, retrying up to 5m...\")\n",
    "        raise\n",
    "\n",
    "def scrape_site(protocol, domain, start_path, extra_scrape_link_callback=None):\n",
    "    scraped_object_ids = set()\n",
    "    stats = {'num_scraped': 0}\n",
    "    \n",
    "    def _get_scraped_object(_type, _referrer, _value, alt=None, title=None):\n",
    "        object_id = '{}:{}'.format(_type, _value)\n",
    "        if object_id in scraped_object_ids:\n",
    "            return None\n",
    "        else:\n",
    "            scraped_object_ids.add(object_id)\n",
    "            return {'type': _type, 'referrer': _referrer, 'value': _value, 'alt': alt or '', 'title': title or ''}\n",
    "    \n",
    "    def _save_scraped_object(obj, content):\n",
    "        assert obj['type'] == 'link'\n",
    "        blob_name = 'btm/scraped_objects/{}'.format(obj['value'].replace('https://', '').replace('http://', ''))\n",
    "        blob = bucket.blob('{}.content'.format(blob_name))\n",
    "        blob.upload_from_string(content)\n",
    "        blob = bucket.blob('{}.json'.format(blob_name))\n",
    "        blob.upload_from_string(json.dumps(obj))\n",
    "    \n",
    "    def _save_image_object(obj):\n",
    "        assert obj['type'] == 'image' and obj['value'] and obj['value'].startswith('http')\n",
    "        r = retry_session_get(obj['value'])\n",
    "        if r.status_code == 200:\n",
    "            blob_name = 'btm/scraped_objects/{}'.format(obj['value'].replace('https://', '').replace('http://', ''))\n",
    "            blob = bucket.blob(blob_name)\n",
    "            blob.upload_from_string(r.content)       \n",
    "    \n",
    "    def _check_num_scraped():\n",
    "        num_scraped = len(scraped_object_ids)\n",
    "        if num_scraped > stats['num_scraped']:\n",
    "            stats['num_scraped'] = num_scraped\n",
    "            print('{} / ?'.format(num_scraped))\n",
    "        return not MAX_OBJECTS or num_scraped < MAX_OBJECTS\n",
    "    \n",
    "    def _scrape(_url, _referrer, **obj_kwargs):\n",
    "        if f'//{domain}' in _url or (extra_scrape_link_callback and extra_scrape_link_callback(_url)):\n",
    "            num_scraped = len(scraped_object_ids)\n",
    "            if num_scraped > stats['num_scraped']:\n",
    "                stats['num_scraped'] = num_scraped\n",
    "                print('{} / ?'.format(num_scraped))\n",
    "            if _referrer == 'root':\n",
    "                yield _get_scraped_object('root', '', _url)\n",
    "            if MAX_OBJECTS < 1 or num_scraped < MAX_OBJECTS:\n",
    "                obj = _get_scraped_object('link', _referrer, _url, **obj_kwargs)\n",
    "                if obj:\n",
    "                    yield obj\n",
    "                    if _url.strip()[-4:].lower() in ['.jpg', '.gif', '.png', '.svg', 'txt', 'xml', 'ico']:\n",
    "                        pass\n",
    "                    elif _url.strip()[-5:].lower() in ['.jpeg']:\n",
    "                        pass\n",
    "                    else:\n",
    "                        try:\n",
    "                            r = retry_session_get(_url)\n",
    "                            _save_scraped_object(obj, r.content)\n",
    "                            for a in r.html.find('a'):\n",
    "                                if a.attrs.get('href'):\n",
    "                                    link = urllib.parse.unquote(a.attrs['href'])\n",
    "                                    yield from _scrape(link, 'link:{}'.format(_url),\n",
    "                                                       alt=a.attrs.get('alt'),\n",
    "                                                       title=a.attrs.get('title'))\n",
    "                            for link in r.html.absolute_links:\n",
    "                                link = urllib.parse.unquote(link)\n",
    "                                yield from _scrape(link, 'link:{}'.format(_url),\n",
    "                                                   alt=a.attrs.get('alt'),\n",
    "                                                   title=a.attrs.get('title'))\n",
    "                            for img in r.html.find('img'):\n",
    "                                if img.attrs.get('src'):\n",
    "                                    img.attrs.get('alt')\n",
    "                                    obj = _get_scraped_object('image', 'link:{}'.format(_url), urllib.parse.unquote(img.attrs['src']),\n",
    "                                                              alt=img.attrs.get('alt'),\n",
    "                                                              title=img.attrs.get('title'))\n",
    "                                    if obj:\n",
    "                                        # _save_image_object(obj)\n",
    "                                        yield obj\n",
    "                                if not _check_num_scraped(): break\n",
    "                        except Exception as e:\n",
    "                            print('{} ({})'.format(_url, _referrer))\n",
    "                            traceback.print_exc()\n",
    "                            yield {'type': 'error', 'referrer': 'link:{}'.format(_url), 'value': str(e)}\n",
    "        \n",
    "    return _scrape(f'{protocol}://{domain}{start_path}', 'root')\n",
    "\n",
    "!{'rm -rf .checkpoints/scraped-site'}\n",
    "\n",
    "Flow(\n",
    "    scrape_site('http', 'www.bitmuna.com', '/', lambda link: 'bitmuna.com' in link),\n",
    "    checkpoint('scraped-site'),\n",
    "    printer(tablefmt='html', num_rows=1),\n",
    ").process()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text-based filter by years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflows import Flow, checkpoint, printer\n",
    "\n",
    "def filter_years(rows):\n",
    "    for row in rows:\n",
    "        ok = False\n",
    "        for year in range(10,47):\n",
    "            for field in ['alt', 'title']:\n",
    "                if '19'+str(year) in (row.get(field) or ''):\n",
    "                    ok=True\n",
    "                    break\n",
    "            if ok: break\n",
    "        if ok: yield row\n",
    "\n",
    "Flow(\n",
    "    checkpoint('scraped-site'),\n",
    "    filter_years,\n",
    "    printer(tablefmt='html', num_rows=1),\n",
    "    checkpoint('scraped-site-filtered-years')\n",
    ").process()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parse album titles and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataflows import delete_fields, add_field, Flow\n",
    "\n",
    "def bitmuna_albums():\n",
    "    \n",
    "    def _bitmuna_albums(rows):\n",
    "        for row in rows:\n",
    "            parts = row['referrer'].split('/')\n",
    "            if len(parts) > 4 and parts[2] == 'www.bitmuna.com':\n",
    "                album_title, album_part_title, album_part_subtitle = '', '', ''\n",
    "                if len(parts) == 5:\n",
    "                    album_title = parts[3]\n",
    "                elif len(parts) == 8 and parts[4] == 'nggallery':\n",
    "                    album_title = parts[3]\n",
    "                    album_part_title = parts[5]\n",
    "                    album_part_subtitle = parts[6]\n",
    "                if album_title and row['value'].lower().endswith('.jpg'):\n",
    "                    yield dict(row, **{\n",
    "                        'album_title': album_title,\n",
    "                        'album_part_title': album_part_title,\n",
    "                        'album_part_subtitle': album_part_subtitle,\n",
    "                        'image': row['value']\n",
    "                    })\n",
    "    \n",
    "    return Flow(\n",
    "        add_field('album_title', 'string'),\n",
    "        add_field('album_part_title', 'string'),\n",
    "        add_field('album_part_subtitle', 'string'),\n",
    "        add_field('image', 'string'),\n",
    "        _bitmuna_albums,\n",
    "        delete_fields(['type', 'referrer', 'value', 'alt']),\n",
    "    )\n",
    "                \n",
    "Flow(\n",
    "    checkpoint('scraped-site-filtered-years'),\n",
    "    bitmuna_albums(),\n",
    "    checkpoint('scraped-site-filtered-years-album-images'),\n",
    ").process()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render HTML Preview of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_TEMPLATE = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<head>\n",
    "    <meta charset=\"utf-8\">\n",
    "</head>\n",
    "<body dir=\"rtl\">\n",
    "{content}\n",
    "</body>\n",
    "\"\"\"\n",
    "\n",
    "IMAGE_TEMPLATE = \"\"\"\n",
    "<div style=\"border:1px solid black;\">\n",
    "    <img src=\"{image}\"/><br/>\n",
    "    <small>{title}</small>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "def render_images_preview(out_path):\n",
    "    \n",
    "    os.makedirs(out_path, exist_ok=True)\n",
    "    \n",
    "    def _render(rows):\n",
    "        albums = {}\n",
    "        for row in rows:\n",
    "            albums.setdefault(row['album_title'], {'rows': []})['rows'].append(row)\n",
    "        album_num = 0\n",
    "        index_content = ''\n",
    "        for album_title, album in albums.items():\n",
    "            album_num += 1\n",
    "            content = '<h1>{}</h1>'.format(album_title)\n",
    "            for row in album['rows']:\n",
    "                content += IMAGE_TEMPLATE.format(**row)\n",
    "            index_content += '<p><a href=\"album-{}.html\">{} ({})</a></p>'.format(album_num, album_title, len(album['rows']))\n",
    "            with open(out_path + '/album-' + str(album_num) + '.html', 'w') as f:\n",
    "                f.write(INDEX_TEMPLATE.format(content=content))\n",
    "            yield {'album_title': album_title}\n",
    "        with open(out_path + '/index.html', 'w') as f:\n",
    "            f.write(INDEX_TEMPLATE.format(content=index_content))\n",
    "    \n",
    "    return _render\n",
    "\n",
    "Flow(\n",
    "    checkpoint('scraped-site-filtered-years-album-images'),\n",
    "    render_images_preview(\n",
    "        '../data/btm-site-filtered-years-albums-preview',\n",
    "    ),\n",
    ").process()[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
